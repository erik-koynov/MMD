{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLULayer(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        pass\n",
    "    \"\"\"\n",
    "    forward(): compute the activation function of the layer\n",
    "    inputs: input_ -> vector -> dot product of the matrix multiplication between the activations of the previos layer\n",
    "                                and the weights of the current layer\n",
    "    outputs: relu -> vector\n",
    "    \"\"\"\n",
    "    def forward(self, input_):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input_\n",
    "        # return the ReLU of the input\n",
    "        relu = np.maximum(input_,0)\n",
    "        return relu\n",
    "    \"\"\"\n",
    "    backward(): compute the derivative of the ReLU activation function w.r.t its input and calculate further the\n",
    "                backward pass by multiplying with the already computed gradient from the functions following the\n",
    "                ReLu in the computational graph of our network\n",
    "    inputs: upstream_gradient -> vector -> the derivative of the LOSS w.r.t. to the current activation \n",
    "            self.input -> vector\n",
    "    outputs: downstream_gradient -> vector -> representing the influence that each input of the current layer\n",
    "                                              has had on the loss \n",
    "    \"\"\"\n",
    "    def backward(self, upstream_gradient):\n",
    "        # compute the derivative of ReLU from upstream_gradient and the stored input of the ReLU Layer\\\n",
    "        # derivative of ReLU w.r.t self.input\n",
    "        ReLu_gradient = np.ones_like(self.input)\n",
    "        \n",
    "        ReLu_gradient[np.where(self.input<=0)] = 0\n",
    "        \n",
    "        # LOSS FUNCTION'S gradient w.r.t. relu's input i.e. the Linear function z(W,b,prev_activation)\n",
    "        downstream_gradient = ReLu_gradient*(upstream_gradient)\n",
    "        print(\"ReLU Downstream: \", downstream_gradient.shape)\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        pass # ReLU is parameter-free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.make_blobs(n_features=5,centers=2)\n",
    "X = data[0]\n",
    "y = data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class OutputLayer(object):\n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "        self.cross_entropy_loss = 0\n",
    "        \n",
    "    \"\"\"\n",
    "    forward(): compute the activation function of the layer\n",
    "    input: logits -> vector -> the dot product of the activations in the penultimate layer and the weights of\n",
    "                               the current layer\n",
    "    output: softmax -> vector\n",
    "    \"\"\"\n",
    "    def forward(self, logits):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = logits\n",
    "        # return the softmax of the input\n",
    "        # axis = 1 because we need the max logit for each datapoint as axis 0 corresponds to the batch dimension\n",
    "        \n",
    "        max_ = np.max(logits,axis=1)[:,None]\n",
    "        sum_ = np.sum(np.exp((logits-max_)),axis=1)[:,None]\n",
    "        softmax = np.exp((logits-max_))/ sum_\n",
    "                      \n",
    "                      \n",
    "        return softmax\n",
    "    \n",
    "    \"\"\"\n",
    "    backward(): compute the derivative of the softmax activation function w.r.t its input and calculate further the\n",
    "                backward pass by multiplying with the already computed gradient from the cost function w.r.t to the\n",
    "                predictions\n",
    "    inputs: predictions -> vector\n",
    "            self.input -> vector\n",
    "            y -> number : the true label\n",
    "    outputs: downstream_gradient -> vector -> representing the influence that each input of the current layer\n",
    "                                              has had on the LOSS (logically if the input was below 0 then its\n",
    "                                              influence on the loss will also be 0)\n",
    "    \"\"\"\n",
    "    def backward(self, predictions, y, return_loss = False):\n",
    "        # return the loss derivative with respect to the stored inputs of the layer \n",
    "        # (use cross-entropy loss and the chain rule for softmax,\n",
    "        #  as derived in the lecture) \n",
    "        print(\"Crossentropy\",predictions.shape)\n",
    "        print(\"true : \",y.shape)\n",
    "        if len(predictions.shape)==1:\n",
    "            predictions = predictions[:,None]\n",
    "        batch_size = predictions.shape[0]\n",
    "        \n",
    "        # cross entropy loss \n",
    "        self.cross_entropy_loss = np.mean(-np.log(predictions[range(batch_size),y]+1e-8)) # to make it numerically stable\n",
    "        \n",
    "        # derivative of loss w.r.t. the logits : vector  \n",
    "        downstream_gradient = predictions\n",
    "        downstream_gradient[range(batch_size),y]-=1\n",
    "        \n",
    "        \n",
    "        # LOSS FUNCTION'S gradient w.r.t. softmax's input i.e. the Linear function z(W,b,prev_activation)\n",
    "        if return_loss:\n",
    "            return downstream_gradient, self.cross_entropy_loss\n",
    "        else: \n",
    "            #downstream_gradient = cross_entropy_derivative.dot(softmax_derivative)\n",
    "            return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        pass # softmax is parameter-free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions\n",
      " [[1.77188792e-01 3.10450888e-04 1.54869884e-06 8.22498227e-01\n",
      "  9.81435359e-07]\n",
      " [3.54743257e-03 9.22843383e-06 2.41559594e-05 9.96021613e-01\n",
      "  3.97570286e-04]]\n",
      "y [0 1]\n",
      "Crossentropy (2, 5)\n",
      "true :  (2,)\n",
      "g [[-8.22811208e-01  3.10450888e-04  1.54869884e-06  8.22498227e-01\n",
      "   9.81435359e-07]\n",
      " [ 3.54743257e-03 -9.99990772e-01  2.41559594e-05  9.96021613e-01\n",
      "   3.97570286e-04]]\n"
     ]
    }
   ],
   "source": [
    "output= OutputLayer(2)\n",
    "p=output.forward(X[:2])\n",
    "print(\"predictions\\n\",p)\n",
    "print(\"y\",y[:2])\n",
    "g=output.backward(p,y[:2])\n",
    "\n",
    "print(\"g\",g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(object):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        self.n_inputs  = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        # randomly initialize weights and intercepts\n",
    "        self.B = np.random.uniform(size=(n_inputs,n_outputs)) # your code here\n",
    "        self.b = np.random.uniform(size=(n_outputs,)) # your code here\n",
    "        self.grad_B = None\n",
    "        self.grad_b = None\n",
    "    \"\"\"\n",
    "    forward(): compute the linear function z(W,b,X) = XW + b which will then be passed as parameter to the \n",
    "               respective activation function\n",
    "    input: input_ -> vector -> the activation of the previos layer (or the input X is this is the first layer)\n",
    "    output: preactivations -> vector : XW + b\n",
    "    \"\"\"\n",
    "    def forward(self, input_):\n",
    "        # remember the input for later backpropagation\n",
    "        if len(input_.shape)==1:\n",
    "            self.input = deepcopy(input_)\n",
    "            self.input = self.input[:,None]\n",
    "        else:\n",
    "            self.input = deepcopy(input_)\n",
    "        # compute the scalar product of input and weights\n",
    "        # (these are the preactivations for the subsequent non-linear layer)\n",
    "        preactivations = self.input.dot(self.B)+self.b\n",
    "        \n",
    "        return preactivations\n",
    "    \n",
    "    \"\"\"\n",
    "    backward(): compute the derivative of the preactivation z(W,b,X) w.r.t its inputs and calculate further the\n",
    "                backward pass by multiplying with the already computed gradient from the cost function w.r.t to the\n",
    "                current activation with the computed gradient of the preactivation w.r.t. to the activation in the \n",
    "                previous layer\n",
    "    inputs: upstream_gradient -> vector -> gradient of LOSS w.r.t. to the current activation\n",
    "            self.B -> matrix -> weights of the current layer\n",
    "            self.b -> vector -> the bias vector\n",
    "    outputs: downstream_gradient -> vector -> representing the influence that each input_ of the current layer\n",
    "                                              (activation of the previous layer) has had on the LOSS\n",
    "    \"\"\"\n",
    "    def backward(self, upstream_gradient):\n",
    "        # compute the derivative of the weights from upstream_gradient and the stored input\n",
    "        \n",
    "        # gradient of the matrix multiplication w.r.t. the biases : equals 1 since all other parameters are\n",
    "        # constants with respect to the bias. Then we calculate the gradient of the LOSS w.r.t. the bias vector \n",
    "        # taking the mean since grad_b is initially a matrix containing the gradient vector for each batch\n",
    "        self.grad_b = np.mean(1 * upstream_gradient, axis = 0)\n",
    "        print(\"input shape of the linear layer : \",self.input.shape)\n",
    "        print(\"upstream shape from softmax : \",upstream_gradient.shape)\n",
    "        print(\"shape of the weights :\",self.B.shape)\n",
    "        # gradient of the matrix multiplication w.r.t. the weights : equals the inputs for each row of \n",
    "        # the weights matrix. We then calculate the gradient of the LOSS w.r.t. each weight in the matrix.\n",
    "        # We apply mean because the initial grad_B contains the gradients w.r.t. to each weight for each batch\n",
    "        self.grad_B = np.mean((self.input[:,None,:] * upstream_gradient[:,:,None]),axis=0)\n",
    "         \n",
    "        # compute the downstream gradient to be passed to the preceding layer\n",
    "        # gradient of the LOSS with respect to the input (the activation of the previous layer)\n",
    "        downstream_gradient = upstream_gradient[:,None].dot(self.B.T)\n",
    "        print(\"downstream gradient shape : \",np.reshape(downstream_gradient,(batch_size,downstream_gradient.shape[-1])).shape)\n",
    "        return np.reshape(downstream_gradient,(batch_size,downstream_gradient.shape[-1]))\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        # update the weights by batch gradient descent\n",
    "        print(\"shape of grad_B\",self.grad_B.shape)\n",
    "        print(\"shape of grad_b\",self.grad_b.shape)\n",
    "        self.B = self.B - learning_rate * self.grad_B.T\n",
    "        self.b = self.b - learning_rate * self.grad_b.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-layer perceptron\n",
    "class MLP(object):\n",
    "    def __init__(self, n_features, layer_sizes):\n",
    "        # constuct a multi-layer perceptron\n",
    "        # with ReLU activation in the hidden layers and softmax output\n",
    "        # (i.e. it predicts the posterior probability of a classification problem)\n",
    "        #\n",
    "        # n_features: number of inputs\n",
    "        # len(layer_size): number of layers\n",
    "        # layer_size[k]: number of neurons in layer k\n",
    "        # (specifically: layer_sizes[-1] is the number of classes)\n",
    "        self.n_layers = len(layer_sizes)\n",
    "        self.layers   = []\n",
    "\n",
    "        # create interior layers (linear + ReLU)\n",
    "        n_in = n_features\n",
    "        for n_out in layer_sizes[:-1]:\n",
    "            self.layers.append(LinearLayer(n_in, n_out))\n",
    "            self.layers.append(ReLULayer())\n",
    "            n_in = n_out\n",
    "\n",
    "        # create last linear layer + output layer\n",
    "        n_out = layer_sizes[-1]\n",
    "        self.layers.append(LinearLayer(n_in, n_out))\n",
    "        self.layers.append(OutputLayer(n_out))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X is a mini-batch of instances\n",
    "        batch_size = X.shape[0]\n",
    "        # flatten the other dimensions of X (in case instances are images)\n",
    "        X = X.reshape(batch_size, -1)\n",
    "\n",
    "        # compute the forward pass\n",
    "        # (implicitly stores internal activations for later backpropagation)\n",
    "        result = X\n",
    "        for layer in self.layers:\n",
    "            result = layer.forward(result)\n",
    "        return result\n",
    "\n",
    "    def backward(self, predictions, true_classes):\n",
    "        # perform backpropagation w.r.t. the prediction for the latest mini-batch X\n",
    "        # first compute the gradient of the LOSS w.r.t. the penultimate activation\n",
    "        dLOSS_dINPUT, current_loss = self.layers[-1].backward(predictions, true_classes, return_loss = True)\n",
    "        print(\"Loss: \",current_loss)\n",
    "        \n",
    "        # we can then calculate the derivatives of the LOSS w.r.t. all other activations \n",
    "        for i in range(len(self.layers)-2, -1, -1):\n",
    "            print(\"layer: \",i)\n",
    "            dLOSS_dINPUT = self.layers[i].backward(dLOSS_dINPUT)\n",
    "            \n",
    "            \n",
    "    def update(self, X, Y, learning_rate):\n",
    "        posteriors = self.forward(X)\n",
    "        self.backward(posteriors, Y)\n",
    "        for layer in self.layers:\n",
    "            layer.update(learning_rate)\n",
    "\n",
    "    def train(self, x, y, n_epochs, batch_size, learning_rate):\n",
    "        N = len(x)\n",
    "        n_batches = N // batch_size\n",
    "        for i in range(n_epochs):\n",
    "            # print(\"Epoch\", i)\n",
    "            # reorder data for every epoch\n",
    "            # (i.e. sample mini-batches without replacement)\n",
    "            permutation = np.random.permutation(N)\n",
    "\n",
    "            for batch in range(n_batches):\n",
    "                # create mini-batch\n",
    "                start = batch * batch_size\n",
    "                x_batch = x[permutation[start:start+batch_size]]\n",
    "                y_batch = y[permutation[start:start+batch_size]]\n",
    "\n",
    "                # perform one forward and backward pass and update network parameters\n",
    "                \n",
    "                self.update(x_batch, y_batch, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.7721637577281478\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  7.552456837348658\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.4161466631183097\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  4.210548034323487\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  7.724913781194338\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  6.046803068705284\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  9.465402140013111\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  7.664849480267054\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.47998804660582833\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  1.3417390418197206\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  4.63564438904558\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.40388318320682337\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  1.1459780260710204\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  2.487177042619067\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  3.1304455158027698\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.17892302425320544\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.19971022193404117\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.3233525477452415\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.5289112129160731\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.8063269275009024\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.28646517983690545\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.39167811576332157\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2585410132225363\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2580506771258493\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.33196685394804476\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.4227828107076472\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.7049737541656856\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  1.6112754753487084\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.48495715506524995\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.7230285583216918\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  1.0400473907257897\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  2.909064355809345\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.41668167740436024\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.30814838997999755\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.263217269857635\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2640671580074869\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2259900207961748\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.18243402281124899\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.33209653878849055\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.38099817772791833\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.27927963556955115\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.23111600429340143\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2504265855063902\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.30658080626510703\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.31534462212515335\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.3213861050793295\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.29898331167919534\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.26632352088436156\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.31889003690427026\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.36732045414366354\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.28059649886511073\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.19594534735562624\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.22247504380127545\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2740976165676525\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.17777476101147663\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.22361632035076107\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2776267258748557\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2233072232492738\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2791350829139425\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.1818356247215935\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2966207085180878\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.1842250779670883\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.22726770582616898\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2410226812589465\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.188693729044853\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2374905932737558\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.24748229311862233\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2934168923766395\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2877788445958698\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.19269079765896607\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2421232900651524\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.29787195391805743\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.09981781184745285\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.25610647418860233\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.12062644369292035\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2091483606596306\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.20106678500898564\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.33898973488914824\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2947274981533004\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.21844418592218748\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.27011038554961986\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.20433315143864106\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.22605865634988614\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.20557131515259294\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.21718782478036733\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.20227032510587872\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.1131707638842799\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.28917304094002455\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.28622572830470316\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2627063823813156\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.19102144012681324\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.30006663125447797\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.16250272280723874\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.21668807827885922\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.1491295664950918\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.15359270536688105\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.27578515438558976\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.3244985701438067\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2988174057606851\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.20346140406532526\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.25236699473098206\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2302288913875536\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2641911586635165\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.21254727507817414\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2607922454439375\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.20172996150622363\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.14066798520427345\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2109636079154261\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.16433752449717784\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2299446933567754\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.14834943003608958\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.18969231951343155\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.31987810130535715\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.23903758166563105\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.18786731147215582\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.24557063158943804\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.17259961231547527\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.16276799949877638\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.29411636565713556\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.1719387160114385\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.15995430401135827\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.15330990077443227\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2092079725341973\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2376734109769825\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.24474082304650704\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.36767684556257507\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.17304731123770442\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.1820607184350368\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.22210457960364874\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.19146228798416282\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.20715547683208352\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.15768247066853885\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.19588929024881607\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.1853292149064751\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.15299673594537766\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2912321286065597\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.17148771328589768\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.1494066464246534\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.18715473999826607\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.20879767585370085\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2143104669919474\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.17662033027029383\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.22322225340196572\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2633181445563133\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.15667018055577045\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.16875439546974147\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.24759949718067684\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.087483973955083\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.27536327775910746\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2195146226417682\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.211954016138895\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.21661098588069155\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.1394883905004681\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2345635658891592\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.25522136672941065\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.19451890470743544\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.18343575795028247\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.19783462793237652\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2116913544127364\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2037933315078707\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.20668319860608708\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.22025602401112493\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.22878891385401798\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.15668858065830635\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.20997869513015796\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2270604079360852\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.21816971950278474\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.15891098533132092\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.25430897032829414\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.216996251081486\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.1640164668454112\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2264026478330987\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.21048873017788913\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.21624607018950828\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.24613390845020444\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.18624829017908875\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.1746930839738466\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.17531347288980295\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.13270791316763703\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.15287222073690349\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2545959513155782\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.2502497679616889\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.21199244959139918\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.24917196400187167\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.17694297484200724\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.18190811356270647\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.17963867695635863\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.16354002077924956\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.12852971800196486\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.18654766484459323\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.19740444879386027\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.22416956250582562\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.16239093335366994\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.20972860059434922\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.16852574377762067\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.173047720437634\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.15936795786231645\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.20801667146946035\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.13311267156466727\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "Crossentropy (100, 2)\n",
      "true :  (100,)\n",
      "Loss:  0.171765674464162\n",
      "layer:  4\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 2)\n",
      "shape of the weights : (30, 2)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  3\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  2\n",
      "input shape of the linear layer :  (100, 30)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (30, 30)\n",
      "downstream gradient shape :  (100, 30)\n",
      "layer:  1\n",
      "ReLU Downstream:  (100, 30)\n",
      "layer:  0\n",
      "input shape of the linear layer :  (100, 2)\n",
      "upstream shape from softmax :  (100, 30)\n",
      "shape of the weights : (2, 30)\n",
      "downstream gradient shape :  (100, 2)\n",
      "shape of grad_B (30, 2)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (30, 30)\n",
      "shape of grad_b (30,)\n",
      "shape of grad_B (2, 30)\n",
      "shape of grad_b (2,)\n",
      "[[0.14230143 0.85769857]\n",
      " [0.99879453 0.00120547]\n",
      " [0.99771144 0.00228856]\n",
      " ...\n",
      " [0.02698857 0.97301143]\n",
      " [0.0179547  0.9820453 ]\n",
      " [0.99811849 0.00188151]]\n",
      "[1 0 0 ... 1 1 0]\n",
      "0 0\n",
      "error rate: 0.0755\n"
     ]
    }
   ],
   "source": [
    "# sorry for the output log -> I needed to know the shapes and I thought it will be nice for you to see them too\n",
    "# for the purposes of correcting :) \n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    # set training/test set size\n",
    "    N = 2000\n",
    "\n",
    "    # create training and test data\n",
    "    X_train, Y_train = datasets.make_moons(N, noise=0.05)\n",
    "    X_test,  Y_test  = datasets.make_moons(N, noise=0.05)\n",
    "    n_features = 2\n",
    "    n_classes  = 2\n",
    "\n",
    "    # standardize features to be in [-1, 1]\n",
    "    offset  = X_train.min(axis=0)\n",
    "    scaling = X_train.max(axis=0) - offset\n",
    "    X_train = ((X_train - offset) / scaling - 0.5) * 2.0\n",
    "    X_test  = ((X_test  - offset) / scaling - 0.5) * 2.0\n",
    "\n",
    "    # set hyperparameters (play with these!)\n",
    "    layer_sizes = [30, 30, n_classes]\n",
    "    n_epochs = 5\n",
    "    batch_size = 100\n",
    "    learning_rate = 0.05\n",
    "\n",
    "    # create network\n",
    "    network = MLP(n_features, layer_sizes)\n",
    "\n",
    "    # train\n",
    "    network.train(X_train, Y_train, 10, batch_size, learning_rate)\n",
    "    \n",
    "    # test\n",
    "    predicted_posteriors = network.forward(X_test)\n",
    "    print(predicted_posteriors)\n",
    "    # determine class predictions from posteriors by winner-takes-all rule\n",
    "    predicted_classes = np.argmax(predicted_posteriors,axis=1)\n",
    "    print(predicted_classes)# your code here\n",
    "    # compute and output the error rate of predicted_classes\n",
    "    print(predicted_classes[1],Y_test[1])\n",
    "    error_rate = np.sum(np.logical_not(np.equal(predicted_classes,Y_test)))/Y_test.shape[0] # your code here\n",
    "    print(\"error rate:\", error_rate)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "error rate: 0.11 [5,5,n_classes] for 10 epochs \n",
    "error rate: 0.1285 [2,2,n_classes] for 10 epochs\n",
    "error rate: 0.159 [3,3,n_classes] for 10 epochs\n",
    "error rate: 0.0755 [30,30,n_classes] for 10 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
